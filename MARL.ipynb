{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionSpace():\n",
    "    \"\"\"Abstract model for a space that is used for the state and action spaces. This class has the\n",
    "    exact same API that OpenAI Gym uses so that integrating with it is trivial.\n",
    "    Please refer to [Gym Documentation](https://gym.openai.com/docs/#spaces)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.actions = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "    \n",
    "    def sample(self, seed=None):\n",
    "        if seed is not None:\n",
    "            return self.actions[np.random.RandomState(seed=seed).randint(low=0, high=len(self.actions))]\n",
    "        return self.actions[np.random.randint(low=0, high=len(self.actions))]\n",
    "\n",
    "    def contains(self, x):\n",
    "        \"\"\"Return boolean specifying if x is a valid member of this space\n",
    "        \"\"\"\n",
    "        return True if x in self.actions else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateSpace():\n",
    "    \"\"\"Abstract model for a space that is used for the state and action spaces. This class has the\n",
    "    exact same API that OpenAI Gym uses so that integrating with it is trivial.\n",
    "    Please refer to [Gym Documentation](https://gym.openai.com/docs/#spaces)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = [(i, j) for j in range(1, 9) for i in range(1, 9)] ## Some are non reachable\n",
    "    \n",
    "    def sample(self, seed=None):\n",
    "        if seed is not None:\n",
    "            return np.random.RandomState(seed=seed).choice(self.states)\n",
    "        return np.random.choice(self.states)\n",
    "\n",
    "    def contains(self, x):\n",
    "        \"\"\"Return boolean specifying if x is a valid member of this space\n",
    "        \"\"\"\n",
    "        return True if x in self.states else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_indices(br):\n",
    "    if br[0] > br[1]:\n",
    "        br = set([0])\n",
    "    elif br[0] < br[1]:\n",
    "        br = set([1])\n",
    "    else:\n",
    "        br = set([0, 1])\n",
    "    return br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nash_equilibrium(bimatrix, return_all=False, choice=\"value\"):\n",
    "    assert(bimatrix.shape == (2, 2, 2))\n",
    "    br = [[0, 0], [0, 0]]\n",
    "    br[0][0] = get_max_indices(bimatrix[:, 0, 0])\n",
    "    br[0][1] = get_max_indices(bimatrix[:, 1, 0])\n",
    "    br[1][0] = get_max_indices(bimatrix[0, :, 1])\n",
    "    br[1][1] = get_max_indices(bimatrix[1, :, 1])\n",
    "    ne = []\n",
    "    for s in [[0, 0], [0, 1], [1, 0], [1, 1]]:\n",
    "        if s[0] in br[0][s[1]] and s[1] in br[1][s[0]]:\n",
    "            ne.append(s)\n",
    "    \n",
    "    ## NE exists\n",
    "    if len(ne) > 0:\n",
    "        ## Multiple NE\n",
    "        if len(ne) > 1:\n",
    "            if not return_all:\n",
    "                if choice == \"value\":\n",
    "                    idx = -1\n",
    "                    value = -10000000\n",
    "                    for i in range(len(ne)):\n",
    "                        if sum(bimatrix[ne[i][0], ne[i][1]]) > value:\n",
    "                            value = sum(bimatrix[ne[i][0], ne[i][1]])\n",
    "                            idx = i\n",
    "                    return [ne[idx][0], ne[idx][1]], bimatrix[ne[idx][0], ne[idx][1]]\n",
    "                ## Random choice\n",
    "                else:\n",
    "                    idx = np.random.randint(low=0, high=len(ne))\n",
    "                    return [ne[idx][0], ne[idx][1]], bimatrix[ne[idx][0], ne[idx][1]]\n",
    "            if return_all:\n",
    "                _all = []\n",
    "                for i in range(len(ne)):\n",
    "                     _all.append([[ne[i][0], ne[i][1]], bimatrix[ne[i][0], ne[i][1]]])\n",
    "                return _all\n",
    "        else:\n",
    "            idx = 0\n",
    "            return [ne[idx][0], ne[idx][1]], bimatrix[ne[idx][0], ne[idx][1]]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Enviroment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        #reward_range = (-np.inf, np.inf)\n",
    "        self.action_space = ActionSpace()\n",
    "        self.state_space = StateSpace() # state_space\n",
    "        self.q_table = {}\n",
    "        for st in self.state_space.states:\n",
    "            self.q_table[st] = np.array([[[0, 0], [0, 0]], [[0, 0], [0, 0]]], dtype=np.float32)\n",
    "        self.info = {}\n",
    "        self.t = 0\n",
    "        self.start_state = (1, 2)\n",
    "        self.current_state = (1, 2)\n",
    "        self._seed = None\n",
    "    \n",
    "    def _next_state_A(self, state_A, action_A):\n",
    "        up = {1:3, 3:6, 6:6, 4:7, 7:7, 2:5, 5:8, 8:8}\n",
    "        right = {1:4, 3:4, 6:7, 4:5, 7:8, 2:8, 5:8, 8:8}\n",
    "        if action_A == 1:\n",
    "            return up[state_A]  \n",
    "        else:\n",
    "            return right[state_A]\n",
    "    \n",
    "    def _next_state_B(self, state_B, action_B):\n",
    "        up = {1:3, 3:6, 6:6, 4:7, 7:7, 2:5, 5:8, 8:8}\n",
    "        left = {1:1, 3:3, 6:6, 4:3, 7:6, 2:4, 5:4, 8:7}\n",
    "        if action_B == 1:\n",
    "            return up[state_B]  \n",
    "        else:\n",
    "            return left[state_B]\n",
    "    \n",
    "    def _is_final_state(self, new_state):\n",
    "        assert(self.state_space.contains(new_state))\n",
    "        ## Co operation: All 3 ways\n",
    "        if new_state[0] == 7 and new_state[1] == 7:\n",
    "            return True\n",
    "        \n",
    "        ## Fight!: All 2 ways\n",
    "        if new_state[0] == 4 and new_state[1] == 4:\n",
    "            return True\n",
    "        \n",
    "        ## A hunts B: Just one way this happens: Sightly tricky\n",
    "        if new_state[0] == 7 and new_state[1] != 7:\n",
    "            return True\n",
    "        \n",
    "        ## B hunts A: Just one way this happens: Slightly tricy\n",
    "        if new_state[0] != 7 and new_state[1] == 7:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    def _reward(self, new_state):\n",
    "        assert(self.state_space.contains(new_state))\n",
    "        ## Co operation\n",
    "        if new_state[0] == 7 and new_state[1] == 7:\n",
    "            return (3, 3)\n",
    "        \n",
    "        if new_state[0] == 4 and new_state[1] == 4:\n",
    "            return (1, 1)\n",
    "        \n",
    "        if new_state[0] == 7 and new_state[1] != 7:\n",
    "            return (4, 0)\n",
    "        \n",
    "        if new_state[0] != 7 and new_state[1] == 7:\n",
    "            return (0, 4)\n",
    "        return (0, 0)\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        # Arguments\n",
    "            action (object): An action provided by the environment.\n",
    "        # Returns\n",
    "            observation (object): Agent's observation of the current environment.\n",
    "            reward (float) : Amount of reward returned after previous action.\n",
    "            done (boolean): Whether the episode has ended, in which case further step() calls will return undefined results.\n",
    "            info (dict): Contains auxiliary diagnostic information (helpful for debugging, and sometimes learning).\n",
    "        \"\"\"\n",
    "        new_state = (self._next_state_A(self.current_state[0], action[0]),\\\n",
    "                     self._next_state_B(self.current_state[1], action[1]))\n",
    "        done = self._is_final_state(new_state)\n",
    "        reward = self._reward(new_state)\n",
    "        self.t += 1\n",
    "        self.info[self.t] = (new_state, reward, done)\n",
    "        self.current_state = new_state\n",
    "        return new_state, reward, done, self.info[self.t]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment and returns an initial observation.\n",
    "        # Returns\n",
    "            observation (object): The initial observation of the space. Initial reward is assumed to be 0.\n",
    "        \"\"\"\n",
    "        self.t = 0\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.)\n",
    "        # Arguments\n",
    "            mode (str): The mode to render with.\n",
    "            close (bool): Close all open renderings.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Override in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        # Returns\n",
    "            Returns the list of seeds used in this env's random number generators\n",
    "        \"\"\"\n",
    "        _seed = seed\n",
    "        return _seed\n",
    "\n",
    "    def configure(self, *args, **kwargs):\n",
    "        \"\"\"Provides runtime configuration to the environment.\n",
    "        This configuration should consist of data that tells your\n",
    "        environment how to run (such as an address of a remote server,\n",
    "        or path to your ImageNet data). It should not affect the\n",
    "        semantics of the environment.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    #def __del__(self):\n",
    "    #    self.close()\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<{} instance>'.format(type(self).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "discount_rate = 1\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Enviroment()\n",
    "q_table = env.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 5\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    for step in range(max_steps_per_episode):\n",
    "        r = np.random.uniform(0, 1)\n",
    "        ## Exploitation\n",
    "        if r > exploration_rate:\n",
    "            ne = nash_equilibrium(q_table[state], choice=\"random\")\n",
    "            if ne is None:\n",
    "                print('Ugh')\n",
    "                break\n",
    "            action = ne[0]\n",
    "        ## Exploration\n",
    "        else:\n",
    "            action = env.action_space.sample(seed=env._seed)\n",
    "            #print(action, end=\" \")\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #print(new_state, reward, done)\n",
    "        ne = nash_equilibrium(q_table[new_state], choice=\"random\")\n",
    "        if ne is None:\n",
    "            print('Ugh')\n",
    "            break\n",
    "        values = ne[1]\n",
    "        update = (1 - alpha) * q_table[state][action[0]][action[1]] + \\\n",
    "                        alpha * (reward + discount_rate * values)\n",
    "        q_table[state][action[0]][action[1]][0] = update[0]\n",
    "        q_table[state][action[0]][action[1]][1] = update[1]\n",
    "        state = new_state\n",
    "        if done == True:\n",
    "            break\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "            (max_exploration_rate - min_exploration_rate)*np.exp(-exploration_decay_rate*episode) \n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0], array([0.99999994, 0.99999994], dtype=float32))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nash_equilibrium(q_table[(1, 2)], return_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 1], array([3., 3.], dtype=float32)],\n",
       " [[1, 0], array([3., 3.], dtype=float32)]]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nash_equilibrium(q_table[(3, 5)], return_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9999991, 0.9999991],\n",
       "        [3.       , 3.       ]],\n",
       "\n",
       "       [[3.       , 3.       ],\n",
       "        [2.9845617, 2.9845617]]], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[(3, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n",
      "[1, 0]\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "for step in range(max_steps_per_episode):\n",
    "    ne = nash_equilibrium(q_table[state], choice=\"random\")\n",
    "    if ne is None:\n",
    "        print('Ugh')\n",
    "        break\n",
    "    action = ne[0]\n",
    "    print(action)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    state = new_state\n",
    "    if done == True:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras_gpu_tensorflow]",
   "language": "python",
   "name": "conda-env-keras_gpu_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
