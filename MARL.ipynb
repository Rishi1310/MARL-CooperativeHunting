{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionSpace():\n",
    "    \"\"\"Abstract model for a space that is used for the state and action spaces. This class has the\n",
    "    exact same API that OpenAI Gym uses so that integrating with it is trivial.\n",
    "    Please refer to [Gym Documentation](https://gym.openai.com/docs/#spaces)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.actions = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "    \n",
    "    def sample(self, seed=None):\n",
    "        if seed is not None:\n",
    "            return self.actions[np.random.RandomState(seed=seed).randint(low=0, high=len(self.actions))]\n",
    "        return self.actions[np.random.randint(low=0, high=len(self.actions))]\n",
    "\n",
    "    def contains(self, x):\n",
    "        \"\"\"Return boolean specifying if x is a valid member of this space\n",
    "        \"\"\"\n",
    "        return True if x in self.actions else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateSpace():\n",
    "    \"\"\"Abstract model for a space that is used for the state and action spaces. This class has the\n",
    "    exact same API that OpenAI Gym uses so that integrating with it is trivial.\n",
    "    Please refer to [Gym Documentation](https://gym.openai.com/docs/#spaces)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = [(i, j) for j in range(1, 9) for i in range(1, 9)] ## Some are non reachable\n",
    "    \n",
    "    def sample(self, seed=None):\n",
    "        if seed is not None:\n",
    "            return np.random.RandomState(seed=seed).choice(self.states)\n",
    "        return np.random.choice(self.states)\n",
    "\n",
    "    def contains(self, x):\n",
    "        \"\"\"Return boolean specifying if x is a valid member of this space\n",
    "        \"\"\"\n",
    "        return True if x in self.states else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enviroment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        #reward_range = (-np.inf, np.inf)\n",
    "        self.action_space = ActionSpace()\n",
    "        self.state_space = StateSpace() # state_space\n",
    "        self.q_table = {}\n",
    "        for st in self.state_space.states:\n",
    "            self.q_table[st] = np.array([[[0, 0], [0, 0]], [[0, 0], [0, 0]]])\n",
    "        self.info = {}\n",
    "        self.t = 0\n",
    "        self.start_state = (1, 2)\n",
    "        self.current_state = (1, 2)\n",
    "        self._seed = None\n",
    "    \n",
    "    def _next_state_A(self, state_A, action_A):\n",
    "        up = {1:3, 3:6, 6:6, 4:7, 7:7, 2:5, 5:8, 8:8}\n",
    "        right = {1:4, 3:4, 6:7, 4:5, 7:8, 2:8, 5:8, 8:8}\n",
    "        if action_A == 1:\n",
    "            return up[state_A]  \n",
    "        else:\n",
    "            return right[state_A]\n",
    "    \n",
    "    def _next_state_B(self, state_B, action_B):\n",
    "        up = {1:3, 3:6, 6:6, 4:7, 7:7, 2:5, 5:8, 8:8}\n",
    "        left = {1:1, 3:3, 6:6, 4:3, 7:6, 2:4, 5:4, 8:7}\n",
    "        if action_B == 1:\n",
    "            return up[state_B]  \n",
    "        else:\n",
    "            return left[state_B]\n",
    "    \n",
    "    def _is_final_state(self, new_state):\n",
    "        assert(self.state_space.contains(new_state))\n",
    "        ## Co operation: All 3 ways\n",
    "        if new_state[0] == 7 and new_state[1] == 7:\n",
    "            return True\n",
    "        \n",
    "        ## Fight!: All 2 ways\n",
    "        if new_state[0] == 4 and new_state[1] == 4:\n",
    "            return True\n",
    "        \n",
    "        ## A hunts B: Just one way this happens: Sightly tricky\n",
    "        if new_state[0] == 7 and new_state[1] != 7:\n",
    "            return True\n",
    "        \n",
    "        ## B hunts A: Just one way this happens: Slightly tricy\n",
    "        if new_state[0] != 7 and new_state[1] == 7:\n",
    "            return True\n",
    "        \n",
    "    def _reward(self, new_state):\n",
    "        assert(self.state_space.contains(new_state))\n",
    "        ## Co operation\n",
    "        if new_state[0] == 7 and new_state[1] == 7:\n",
    "            return (3, 3)\n",
    "        \n",
    "        if new_state[0] == 4 and new_state[1] == 4:\n",
    "            return (1, 1)\n",
    "        \n",
    "        if new_state[0] == 7 and new_state[1] != 7:\n",
    "            return (4, 0)\n",
    "        \n",
    "        if new_state[0] != 7 and new_state[1] == 7:\n",
    "            return (0, 4)\n",
    "        return (0, 0)\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        # Arguments\n",
    "            action (object): An action provided by the environment.\n",
    "        # Returns\n",
    "            observation (object): Agent's observation of the current environment.\n",
    "            reward (float) : Amount of reward returned after previous action.\n",
    "            done (boolean): Whether the episode has ended, in which case further step() calls will return undefined results.\n",
    "            info (dict): Contains auxiliary diagnostic information (helpful for debugging, and sometimes learning).\n",
    "        \"\"\"\n",
    "        new_state = (self._next_state_A(self.current_state[0], action[0]),\\\n",
    "                     self._next_state_B(self.current_state[1], action[1]))\n",
    "        done = self._is_final_state(new_state)\n",
    "        reward = self._reward(new_state)\n",
    "        self.t += 1\n",
    "        self.info[self.t] = (new_state, reward, done)\n",
    "        self.current_state = new_state\n",
    "        return new_state, reward, done, self.info[self.t]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment and returns an initial observation.\n",
    "        # Returns\n",
    "            observation (object): The initial observation of the space. Initial reward is assumed to be 0.\n",
    "        \"\"\"\n",
    "        self.t = 0\n",
    "        self.info = {}\n",
    "        self.current_state = self.start_state\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.)\n",
    "        # Arguments\n",
    "            mode (str): The mode to render with.\n",
    "            close (bool): Close all open renderings.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Override in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        # Returns\n",
    "            Returns the list of seeds used in this env's random number generators\n",
    "        \"\"\"\n",
    "        _seed = seed\n",
    "        return _seed\n",
    "\n",
    "    def configure(self, *args, **kwargs):\n",
    "        \"\"\"Provides runtime configuration to the environment.\n",
    "        This configuration should consist of data that tells your\n",
    "        environment how to run (such as an address of a remote server,\n",
    "        or path to your ImageNet data). It should not affect the\n",
    "        semantics of the environment.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    #def __del__(self):\n",
    "    #    self.close()\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<{} instance>'.format(type(self).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Enviroment()\n",
    "q_table = env.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0) (3, 4) (0, 0) None\n",
      "(0, 1) (4, 7) (0, 4) True\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1\n",
    "max_steps_per_episode = 5\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    for step in range(max_steps_per_episode):\n",
    "        #r = np.random.uniform(0, 1)\n",
    "        r = 0\n",
    "        exploration_rate = 0.5\n",
    "        if r > exploration_rate:\n",
    "            #action = nash_equilibrium(q_table[state])\n",
    "            pass\n",
    "        else:\n",
    "            action = env.action_space.sample(seed=env._seed)\n",
    "            #print(action, end=\" \")\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        print(new_state, reward, done)\n",
    "        q_table[state][action[0]][action[1]] = (1 - alpha) * q_table[state][action[0]][action[1]] + \\\n",
    "                        alpha * (reward + discount_rate * nash_equilibrium(q_table[new_state]))\n",
    "        state = new_state\n",
    "        if done == True:\n",
    "            break\n",
    "    #exploration_rate etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras_gpu_tensorflow]",
   "language": "python",
   "name": "conda-env-keras_gpu_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
